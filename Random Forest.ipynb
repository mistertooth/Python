{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importance Rank Bar Chart\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html\n",
    "http://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regression.html#example-ensemble-plot-gradient-boosting-regression-py"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "\n",
    "\n",
    "#Lets set up a training dataset.  We'll make 100 entries, each with 19 features and\n",
    "#each row classified as either 0 and 1.  We'll control the first 3 features to artificially\n",
    "#set the first 3 features of rows classified as \"1\" to a set value, so that we know these are the \"important\" features.  If we do it right, the model should point out these three as important.  \n",
    "#The rest of the features will just be noise.\n",
    "train_data = [] ##must be all floats.\n",
    "for x in range(100):\n",
    "    line = []\n",
    "    if random.random()>0.5:\n",
    "        line.append(1.0)\n",
    "        #Let's add 3 features that we know indicate a row classified as \"1\".\n",
    "        line.append(.77)\n",
    "        line.append(.33)\n",
    "        line.append(.55)\n",
    "        for x in range(16):#fill in the rest with noise\n",
    "            line.append(random.random())\n",
    "    else:\n",
    "        #this is a \"0\" row, so fill it with noise.\n",
    "        line.append(0.0)\n",
    "        for x in range(19):\n",
    "            line.append(random.random())        \n",
    "    train_data.append(line)\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "\n",
    "# Create the random forest object which will include all the parameters\n",
    "# for the fit.  Make sure to set compute_importances=True\n",
    "Forest = RandomForestClassifier(n_estimators = 100, compute_importances=True)\n",
    "\n",
    "# Fit the training data to the training output and create the decision\n",
    "# trees.  This tells the model that the first column in our data is the classification,\n",
    "# and the rest of the columns are the features.\n",
    "Forest = Forest.fit(train_data[0::,1::],train_data[0::,0])\n",
    "\n",
    "#now you can see the importance of each feature in Forest.feature_importances_\n",
    "# these values will all add up to one.  Let's call the \"important\" ones the ones that are above average.\n",
    "important_features = []\n",
    "for x,i in enumerate(Forest.feature_importances_):\n",
    "    if i>np.average(Forest.feature_importances_):\n",
    "        important_features.append(str(x))\n",
    "print 'Most important features:',', '.join(important_features)\n",
    "#we see that the model correctly detected that the first three features are the most important, just as we expected!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Exploration\n",
    "https://www.kaggle.com/cast42/santander-customer-satisfaction/exploring-features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-output DEcision Tree Regression\n",
    "http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression_multioutput.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "#from textblob import TextBlob\n",
    "import pandas\n",
    "import sklearn\n",
    "import cPickle\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import StratifiedKFold, cross_val_score, train_test_split \n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.learning_curve import learning_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spam Filtering code example\n",
    "http://radimrehurek.com/data_science_python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Python\n",
    "http://www.analyticsvidhya.com/blog/2015/09/random-forest-algorithm-multiple-challenges/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-variate Linear Regression\n",
    "https://github.com/mistertooth/DAT4/blob/master/notebooks/08_linear_regression.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Factorizing, Scaling, Binning\n",
    "http://www.ultravioletanalytics.com/2014/11/05/kaggle-titanic-competition-part-iii-variable-transformations/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKLearn Standardization, Scaling, Binarization, Polynomial Features\n",
    "http://scikit-learn.org/stable/modules/preprocessing.html#normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means\n",
    "https://github.com/jakevdp/sklearn_pycon2015/blob/master/notebooks/04.2-Clustering-KMeans.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Step by Step\n",
    "https://github.com/justmarkham/DAT8/blob/master/notebooks/17_decision_trees.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn Feature Selection\n",
    "http://featureselection.asu.edu/tutorial.php"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with Python\n",
    "http://connor-johnson.com/2014/02/18/linear-regression-with-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Andrew Ng in Python\n",
    "https://github.com/kaleko/CourseraML\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts & Implementation\n",
    "### similarity matrix\n",
    "### collaborative filtering\n",
    "### linear regression\n",
    "https://dataaspirant.com/for-beginners/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Value\n",
    "https://en.wikipedia.org/wiki/P-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All Statistical Test : T-Test, Chi2-Test\n",
    "http://www.ats.ucla.edu/stat/stata/whatstat/whatstat.htm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Variance Trade-off\n",
    "http://machinelearningmastery.com/gentle-introduction-to-the-bias-variance-trade-off-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms by similarity\n",
    "http://machinelearningmastery.com/a-tour-of-machine-learning-algorithms/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R melt and reshape data\n",
    "http://www.r-statistics.com/2012/01/aggregation-and-restructuring-data-from-r-in-action/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Statistical Test SciPi\n",
    "http://www.randalolson.com/2012/08/06/statistical-analysis-made-easy-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Binning Data\n",
    "http://chrisalbon.com/python/pandas_binning_data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature selection \n",
    "http://blog.datadive.net/selecting-good-features-part-i-univariate-selection/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximul Multual Information Score\n",
    "http://minepy.sourceforge.net/docs/1.0.0/python.html\n",
    "http://www.r-bloggers.com/maximal-information-coefficient-part-ii/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Widespace Recommender System\n",
    "https://docs.google.com/document/d/1NrnvDBLFUO3hsagQ5HezMCf6A9ES4WwgHhF0Fu42-24/edit#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Decision Tree Jupyter Step by Step\n",
    "http://nbviewer.jupyter.org/github/rhiever/Data-Analysis-and-Machine-Learning-Projects/blob/master/example-data-science-notebook/Example%20Machine%20Learning%20Notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## SKLearn Notebooks\n",
    "https://github.com/jakevdp/sklearn_pycon2015/tree/master/notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SKlearn RandomForest Py\n",
    "http://www.agcross.com/2015/02/random-forests-in-python-with-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "http://www.ultravioletanalytics.com/2014/12/01/kaggle-titanic-competition-part-vii-random-forests-and-feature-importance/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning for hackers python\n",
    "http://slendermeans.org/ml4h-ch2-p1.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPi Complelte Statistical Function list\n",
    "http://docs.scipy.org/doc/scipy/reference/stats.html"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Normality is a requirement for the chi square test that a variance equals a specified value but there are many tests that are called chi-square because their asymptotic null distribution is chi-square such as the chi-square test for independence in contingency tables and the chi square goodness of fit test. Neither of these tests require normality. This agrees with Peter Ellis' comment.\n",
    "\n",
    "Regarding your question when specific parametric assumptions are not made (normality being just one such assumption) there are nonparametric procedures (rank tests, permutation tests and the bootstrap) that can be applied sith more generality. In regression, robust regression is an alternative to ordinary least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Analysis Cookbook python\n",
    "http://nbviewer.jupyter.org/github/ipython-books/cookbook-code/tree/master/notebooks/chapter07_stats/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kaggle SKlearn video tutorial\n",
    "http://blog.kaggle.com/tag/scikit-learn-tutorial-series/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rules for Dimensionality Reduction\n",
    "http://www.analyticsvidhya.com/blog/2015/07/dimension-reduction-methods/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Notebook\n",
    "http://nbviewer.jupyter.org/gist/justmarkham/6d5c061ca5aee67c4316471f8c2ae976"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval SciPi\n",
    "http://stackoverflow.com/questions/15033511/compute-a-confidence-interval-from-sample-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Tests with SciPi\n",
    "http://docs.scipy.org/doc/scipy/reference/tutorial/stats.html#t-test-and-ks-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## P-Value\n",
    "http://www.dummies.com/how-to/content/what-a-pvalue-tells-you-about-statistical-data.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SciPi Tutorial\n",
    "http://docs.scipy.org/doc/scipy-0.14.0/reference/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Measure Python \n",
    "https://dataaspirant.com/2015/04/11/five-most-popular-similarity-measures-implementation-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender System Python\n",
    "http://muricoca.github.io/crab/tutorial.html#introducing-recommendation-engines   \n",
    "\n",
    "http://www.salemmarafi.com/code/collaborative-filtering-with-python/  \n",
    "\n",
    "http://blogs.gartner.com/martin-kihn/how-to-build-a-recommender-system-in-python/\n",
    "\n",
    "https://github.com/python-recsys/recsys-tutorial/blob/master/tutorial/0-Introduction-to-Non-Personalized-Recommenders.ipynb\n",
    "\n",
    "https://spark.apache.org/docs/1.4.0/api/python/pyspark.mllib.html#module-pyspark.mllib.recommendation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis Tools, ANOVA, Chi2, Correlation COURSERA\n",
    "https://www.coursera.org/learn/data-analysis-tools/home/week/4\n",
    "## Machine Learning Python, Decision Tree, Random Forests, Lasso Regresion, K-Means Cluster\n",
    "https://www.coursera.org/learn/machine-learning-data-analysis/home/week/4\n",
    "## Inferential Statistics in R\n",
    "https://www.coursera.org/learn/statistical-inference/home/week/2\n",
    "## Exploratory Analysis in R\n",
    "https://www.coursera.org/learn/exploratory-data-analysis/home/week/1\n",
    "## Regression\n",
    "https://www.coursera.org/learn/ml-regression/home/week/1\n",
    "## Marketing Analytics in R\n",
    "https://www.coursera.org/learn/foundations-marketing-analytics/home/week/1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Machine Learning Libraries\n",
    "https://www.cbinsights.com/blog/python-tools-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Robust Regression Python\n",
    "http://twiecki.github.io/blog/2013/08/27/bayesian-glms-2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas & Vincent\n",
    "http://wrobstory.github.io/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
